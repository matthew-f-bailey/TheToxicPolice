{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pyenv\\DS\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "import csv\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Variable for location of the tweets.csv file\n",
    "INPUTFILE_PATH = \"Tweets.csv\"\n",
    "tweets = []\n",
    "train_tweets =[]\n",
    "test_tweets = []\n",
    "sentiment_class = set()\n",
    "tweet_sent_class= []\n",
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenizer(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens = [porter.stem(token.lower()) for token in tokens if not token.lower() in stop_words]\n",
    "    return tokens\n",
    "i = 0\n",
    "with open(INPUTFILE_PATH, 'r', encoding=\"utf8\") as csvfile:\n",
    "    tweetreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in tweetreader:\n",
    "        # For skipping the headerline\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        # tweets will contain the tweet text \n",
    "        tweets.append(tokenizer(row[10]))\n",
    "        tweet_sent_class.append(row[1])\n",
    "        sentiment_class.add(row[1])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {}\n",
    "for index, class_name in enumerate(sentiment_class):\n",
    "    class_dict[class_name] = index\n",
    "vocab = {}\n",
    "vocab_index = 0\n",
    "for tokens in tweets:\n",
    "    for key, token in enumerate(tokens):\n",
    "        #all_tokens.add(token)\n",
    "        if token not in vocab:\n",
    "            vocab[token] = vocab_index\n",
    "            vocab_index += 1\n",
    "#train test split\n",
    "train_tweets = tweets[:9000]\n",
    "test_tweets = tweets[9000:]\n",
    "def map_word_vocab(sentence):\n",
    "    idxs = [vocab[w] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "def map_class(sentiment):\n",
    "    return torch.tensor([class_dict[sentiment]], dtype=torch.long)\n",
    "def prepare_sequence(sentence):\n",
    "    # create the input feature vector\n",
    "    input = map_word_vocab(sentence)\n",
    "    return input\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, input_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, word, hidden):\n",
    "        embeds = self.word_embeddings(word)\n",
    "        combined = torch.cat((embeds.view(1, -1), hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "# creating an instance of RNN\n",
    "rnn = RNN(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(sentiment_class))\n",
    "# Setting the loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finnished epoch 0.0%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Finnished epoch \" + str(epoch / 30 * 100)  + \"%\")\n",
    "    for i in range(len(train_tweets)):\n",
    "        sentence = train_tweets[i]\n",
    "        sent_class = tweet_sent_class[i]\n",
    "# Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "# Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = rnn.init_hidden()\n",
    "        rnn.zero_grad()\n",
    "# Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence)\n",
    "        target_class = map_class(sent_class)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        for i in range(len(sentence_in)):\n",
    "            class_scores, hidden = rnn(sentence_in[i], hidden)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(class_scores, target_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentiment_class from set to list\n",
    "sentiment_class = list(sentiment_class)\n",
    "\n",
    "y_pred = []\n",
    "y_actual = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_tweets)):\n",
    "        sentence = test_tweets[i]\n",
    "        sent_class = tweet_sent_class[9000+i]\n",
    "        inputs = prepare_sequence(sentence)\n",
    "        hidden = rnn.init_hidden()\n",
    "        for i in range(len(inputs)):\n",
    "            class_scores, hidden = rnn(inputs[i], hidden)\n",
    "        # for word i. The predicted tag is the maximum scoring tag.\n",
    "        y_pred.append(sentiment_class[((class_scores.max(dim=1)[1].numpy()))[0]])\n",
    "        y_actual.append(str(sent_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051d10de7a63195d3b90ca6c086cd89425cbfaf1dc499c43c0ed1916274eb34c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
