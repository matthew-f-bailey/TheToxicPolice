{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pyenv\\DS\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove stopwords, perform stemming and vectorize\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned ready to be ML'd into next week\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up stopword and stem vars\n",
    "    swords = '|'.join(set(stopwords.words('english')))\n",
    "    swords_re = f\"\\\\b({swords})\\\\b\"\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Remove stopwords\n",
    "    def remove_words(text):\n",
    "        text = re.sub(swords_re, \"\", text)  # remove all stopwords\n",
    "        text = re.sub(f\"[{string.punctuation}]\", \"\", text) # remove all punc\n",
    "        text = re.sub(\"[0-9]+\", \"\", text) # remove all numbers\n",
    "        text = re.sub(\"[^a-zA-Z ]\", \"\", text) # remove all non alpha\n",
    "        text = re.sub(\" +\", \" \", text) # keep everything at 1 space\n",
    "        text = text.replace(r'\\n', '')\n",
    "        return text\n",
    "    df[\"comment_core\"] = df[\"comment_text\"].apply(remove_words)\n",
    "\n",
    "    # Keep only stems\n",
    "    def stem_words(text):\n",
    "        words = text.split(' ')\n",
    "        # Very few words this long\n",
    "        stems = [stemmer.stem(word) for word in words if len(word)<=14]\n",
    "        return \" \".join(stems)\n",
    "    df[\"comment_core_stem\"] = df[\"comment_core\"].apply(stem_words)\n",
    "\n",
    "    # Get tokenizer\n",
    "    tokenizer = Tokenizer(num_words=20_000)\n",
    "    tokenizer.fit_on_texts(df[\"comment_core_stem\"])\n",
    "\n",
    "    # Convert text to vectors and pad\n",
    "    comments_int = tokenizer.texts_to_sequences(df[\"comment_core_stem\"])\n",
    "    comments_pad = pad_sequences(comments_int, maxlen=687)#(max([len(x) for x in comments_int])//2))\n",
    "    comments_pad = pd.DataFrame(comments_pad)\n",
    "\n",
    "    comments = df[[\"comment_text\",\"comment_core_stem\"]]\n",
    "    cleaned = df.drop([\"id\", \"comment_text\", \"comment_core\", \"comment_core_stem\"], axis=1)\n",
    "\n",
    "    return cleaned.join(comments_pad), comments, tokenizer\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        self.df = df\n",
    "        X = df.drop(targets, axis=1)\n",
    "        y = df[targets]\n",
    "        self.y = torch.from_numpy(y.values).to(device)\n",
    "        self.X = torch.from_numpy(X.values).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "cleaned, comments, tokenizer = preprocess(df)\n",
    "vocabulary = tokenizer.index_word\n",
    "train_data, test_data, _, _ = train_test_split(cleaned, cleaned, test_size=0.2, random_state=2022)\n",
    "train_data, valid_data, _, _ = train_test_split(train_data, train_data, test_size=0.2, random_state=2022)\n",
    "train_dataset = CommentDataset(train_data)\n",
    "valid_dataset = CommentDataset(valid_data)\n",
    "test_dataset = CommentDataset(test_data)\n",
    "\n",
    "del df, tokenizer, train_data, test_data, valid_data, stopwords, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.i2h = torch.nn.Linear(embed_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(embed_size + hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        # print(f\"Input size: {input.size()}\")\n",
    "        # print(f\"Hidden Size: {hidden.size()}\")\n",
    "        embeds = self.word_embeddings(input)\n",
    "        # print(f\"Embedded size: {embeds.size()}\")\n",
    "        combined = torch.cat((embeds.view(1, -1), hidden), 1)\n",
    "        # print(f\"Combined size: {combined.size()}\")\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Vocab Size: 185669\n",
      "RNN(\n",
      "  (word_embeddings): Embedding(185669, 48)\n",
      "  (i2h): Linear(in_features=112, out_features=64, bias=True)\n",
      "  (i2o): Linear(in_features=112, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Hyperparams\n",
    "learning_rate = 0.01\n",
    "n_hidden = 64\n",
    "embed_size = 48\n",
    "rnn = RNN(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=n_hidden,\n",
    "    output_size=6,\n",
    "    vocab_size=len(vocabulary)\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "print(f\"Vocab Size: {len(vocabulary)}\")\n",
    "print(rnn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "# train_feats, train_labels = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on epoch: 1\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 2.327928066253662\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(2.1802, device='cuda:0')\n",
      "Starting on epoch: 2\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 2.3719096183776855\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.5853, device='cuda:0')\n",
      "Starting on epoch: 3\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 1.4840929508209229\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.7042, device='cuda:0')\n",
      "Starting on epoch: 4\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 2.2166247367858887\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(2.4714, device='cuda:0')\n",
      "Starting on epoch: 5\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 1.5216482877731323\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.7088, device='cuda:0')\n",
      "Starting on epoch: 6\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 1.5628044605255127\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.7217, device='cuda:0')\n",
      "Starting on epoch: 7\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 1.7241811752319336\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.5538, device='cuda:0')\n",
      "Starting on epoch: 8\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 2.513911008834839\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.6936, device='cuda:0')\n",
      "Starting on epoch: 9\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 1.7781391143798828\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(1.6734, device='cuda:0')\n",
      "Starting on epoch: 10\n",
      "\tStarting on step: 1\n",
      "\t\tStarting on point: 0 of 32\n",
      "Loss: 2.108516216278076\n",
      "Calculating Validation Loss\n",
      "Total loss for epoch: tensor(2.0421, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "epoch_loss = []\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f'Starting on epoch: {epoch+1}')\n",
    "    for step, (X, y) in enumerate(train_dataloader):\n",
    "        print(f'\\tStarting on step: {step+1}')\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if i%16==0:\n",
    "                print(f'\\t\\tStarting on point: {i} of {len(X)}')\n",
    "            sentence = X[i]\n",
    "            targets = y[i]\n",
    "\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            hidden = rnn.init_hidden()\n",
    "            rnn.zero_grad()\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            for i in range(len(sentence)):\n",
    "                class_scores, hidden = rnn(sentence[i].to(device), hidden.to(device))\n",
    "\n",
    "            # Class scores are for the last node\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(class_scores[0].to(device), targets.to(device))\n",
    "            print(f\"Training Loss: {loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            break\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Calculate validation at each epoch\n",
    "        print('\\tCalculating Validation Loss')\n",
    "        for step, (X, y) in enumerate(valid_dataloader):\n",
    "            if step%100!=0:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                sentence = X[i]\n",
    "                targets = y[i]\n",
    "\n",
    "                for i in range(len(sentence)):\n",
    "                    class_scores, _ = rnn(sentence[i].to(device), hidden.to(device))\n",
    "\n",
    "                loss = loss_function(class_scores[0].to(device), targets.to(device))\n",
    "                val_losses.append(loss)\n",
    "            print(f\"\\tGuess: {class_scores}/{targets}\")\n",
    "\n",
    "        print('Total loss for epoch:', loss)\n",
    "        epoch_loss.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[-1.7920, -1.8169, -1.5521, -1.9179, -2.6748, -1.4139]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Convert the sentiment_class from set to list\n",
    "y_pred = []\n",
    "y_actual = []\n",
    "with torch.no_grad():\n",
    "    for step, (X, y) in enumerate(test_dataloader):\n",
    "        for i in range(len(X)):\n",
    "            smp = 14\n",
    "            sentence = X[smp]\n",
    "            targets = y[smp]\n",
    "            hidden = rnn.init_hidden()\n",
    "            for i in range(len(sentence)):\n",
    "                class_scores, hidden = rnn(sentence[i].to(device), hidden.to(device))\n",
    "            print(targets)\n",
    "            print(class_scores)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051d10de7a63195d3b90ca6c086cd89425cbfaf1dc499c43c0ed1916274eb34c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
