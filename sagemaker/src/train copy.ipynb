{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pyenv\\DS\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove stopwords, perform stemming and vectorize\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned ready to be ML'd into next week\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up stopword and stem vars\n",
    "    swords = '|'.join(set(stopwords.words('english')))\n",
    "    swords_re = f\"\\\\b({swords})\\\\b\"\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Remove stopwords\n",
    "    def remove_words(text):\n",
    "        text = re.sub(swords_re, \"\", text)  # remove all stopwords\n",
    "        text = re.sub(f\"[{string.punctuation}]\", \"\", text) # remove all punc\n",
    "        text = re.sub(\"[0-9]+\", \"\", text) # remove all numbers\n",
    "        text = re.sub(\"[^a-zA-Z ]\", \"\", text) # remove all non alpha\n",
    "        text = re.sub(\" +\", \" \", text) # keep everything at 1 space\n",
    "        text = text.replace(r'\\n', '')\n",
    "        return text\n",
    "    df[\"comment_core\"] = df[\"comment_text\"].apply(remove_words)\n",
    "\n",
    "    # Keep only stems\n",
    "    def stem_words(text):\n",
    "        words = text.split(' ')\n",
    "        # Very few words this long\n",
    "        stems = [stemmer.stem(word) for word in words if len(word)<=14]\n",
    "        return \" \".join(stems)\n",
    "    df[\"comment_core_stem\"] = df[\"comment_core\"].apply(stem_words)\n",
    "\n",
    "    # Get tokenizer\n",
    "    tokenizer = Tokenizer(num_words=20_000)\n",
    "    tokenizer.fit_on_texts(df[\"comment_core_stem\"])\n",
    "\n",
    "    # Convert text to vectors and pad\n",
    "    comments_int = tokenizer.texts_to_sequences(df[\"comment_core_stem\"])\n",
    "    comments_pad = pad_sequences(comments_int, maxlen=687)#(max([len(x) for x in comments_int])//2))\n",
    "    comments_pad = pd.DataFrame(comments_pad)\n",
    "\n",
    "    comments = df[[\"comment_text\",\"comment_core_stem\"]]\n",
    "    cleaned = df.drop([\"id\", \"comment_text\", \"comment_core\", \"comment_core_stem\"], axis=1)\n",
    "\n",
    "    return cleaned.join(comments_pad), comments, tokenizer\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        y = self.df.iloc[idx][targets]\n",
    "        X = self.df.drop(targets, axis=1).iloc[idx]\n",
    "        y = torch.from_numpy(y.values)\n",
    "        X = torch.from_numpy(X.values)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "cleaned, comments, tokenizer = preprocess(df)\n",
    "vocabulary = tokenizer.index_word\n",
    "train_data, test_data, _, _ = train_test_split(cleaned, cleaned, test_size=0.2, random_state=2022)\n",
    "train_data, valid_data, _, _ = train_test_split(train_data, train_data, test_size=0.2, random_state=2022)\n",
    "train_dataset = CommentDataset(train_data)\n",
    "valid_dataset = CommentDataset(valid_data)\n",
    "test_dataset = CommentDataset(test_data)\n",
    "\n",
    "del df, tokenizer, train_data, test_data, valid_data, stopwords, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.i2h = torch.nn.Linear(embed_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(embed_size + hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        print(f\"Input size: {input.size()}\")\n",
    "        print(f\"Hidden Size: {hidden.size()}\")\n",
    "        embeds = self.word_embeddings(input)\n",
    "        print(f\"Embedded size: {embeds.size()}\")\n",
    "        combined = torch.cat((embeds.view(1, -1), hidden), 1)\n",
    "        print(f\"Combined size: {combined.size()}\")\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Vocab Size: 185669\n",
      "RNN(\n",
      "  (word_embeddings): Embedding(185669, 100)\n",
      "  (i2h): Linear(in_features=228, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=228, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "Finnished epoch 0.0%\n",
      "Input size: torch.Size([687])\n",
      "Hidden Size: torch.Size([1, 128])\n",
      "Embedded size: torch.Size([687, 100])\n",
      "Combined size: torch.Size([1, 68828])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x68828 and 228x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matt\\Documents\\GitHub\\TheToxicPolice\\sagemaker\\src\\train copy.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=35'>36</a>\u001b[0m \u001b[39m# Step 2. Get our inputs ready for the network, that is, turn them into\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=36'>37</a>\u001b[0m \u001b[39m# Tensors of word indices.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=37'>38</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=38'>39</a>\u001b[0m \u001b[39m# Step 3. Run our forward pass.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=40'>41</a>\u001b[0m     class_scores, hidden \u001b[39m=\u001b[39m rnn(X[i]\u001b[39m.\u001b[39;49mto(device), hidden\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=42'>43</a>\u001b[0m \u001b[39m# Class scores are for the last node\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=43'>44</a>\u001b[0m \u001b[39m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=44'>45</a>\u001b[0m \u001b[39m#  calling optimizer.step()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000007?line=45'>46</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(class_scores, y)\n",
      "File \u001b[1;32mc:\\pyenv\\DS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Matt\\Documents\\GitHub\\TheToxicPolice\\sagemaker\\src\\train copy.ipynb Cell 4'\u001b[0m in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000005?line=16'>17</a>\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embeds\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), hidden), \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000005?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCombined size: \u001b[39m\u001b[39m{\u001b[39;00mcombined\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000005?line=18'>19</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mi2h(combined)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000005?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi2o(combined)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matt/Documents/GitHub/TheToxicPolice/sagemaker/src/train%20copy.ipynb#ch0000005?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(output)\n",
      "File \u001b[1;32mc:\\pyenv\\DS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\pyenv\\DS\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/pyenv/DS/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x68828 and 228x128)"
     ]
    }
   ],
   "source": [
    "# Set Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Hyperparams\n",
    "learning_rate = 0.005\n",
    "n_hidden = 128\n",
    "rnn = RNN(\n",
    "    embed_size=100,\n",
    "    hidden_size=128,\n",
    "    output_size=6,\n",
    "    vocab_size=len(vocabulary)\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "print(f\"Vocab Size: {len(vocabulary)}\")\n",
    "print(rnn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "# train_feats, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Finnished epoch \" + str(epoch / 30 * 100)  + \"%\")\n",
    "    for step, (X, y) in enumerate(train_dataloader):\n",
    "\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        hidden = rnn.init_hidden()\n",
    "        rnn.zero_grad()\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        for i in range(len(X)):\n",
    "            class_scores, hidden = rnn(X[i].to(device), hidden.to(device))\n",
    "\n",
    "        # Class scores are for the last node\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(class_scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051d10de7a63195d3b90ca6c086cd89425cbfaf1dc499c43c0ed1916274eb34c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
